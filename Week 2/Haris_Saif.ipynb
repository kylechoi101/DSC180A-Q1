{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Preprocessing with Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.run(['git', 'pull'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(['git', 'add', 'Haris_Saif.ipynb'])\n",
    "subprocess.run(['git', 'commit', '-m', 'regex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_in = pd.read_csv(\"memos.csv\")\n",
    "df = df_in#.sample(100_000).copy()\n",
    "row_count = df.size\n",
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['memo'].sample(25).sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Regex Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_LIST = [ \n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \n",
    "    \"(?<!\\.)CO(?!['`])\", # Negative lookbehind/ahead for CO (e.g., not .CO or COSTCO) \n",
    "    \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \"HI\", \"IA\", \n",
    "    \"ID\", \"IL\", \n",
    "    \"IN(?!\\\\s+N\\\\s+OUT\\\\s+BURGER)\", # Negative lookahead for IN (not IN N OUT BURGER) \n",
    "    \"KS\", \"KY\", \n",
    "    \"(?<!['`])LA(?!\\\\s+HACIENDA|\\\\s+FITNESS|\\\\s+LA'S|['`])\", # Negative lookaheads for LA \n",
    "    \"MA\", \"MD\", \n",
    "    \"ME(?!\\\\s+DIA)\", # Negative lookahead for ME (not ME DIA) \n",
    "    \"MI\", \"MN\", \"MO(?!['`])\", \n",
    "    \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \n",
    "    \"OH\", \"OK\", \"OR\", \n",
    "    \"PA(?!['`])\", \n",
    "    \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \n",
    "    \"WI\", \"WV\", \"WY\" \n",
    "] \n",
    "STATE_REGEX = r\"\\b(\" + \"|\".join(STATE_LIST) + r\")\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY_LIST = [\n",
    "    \"SAN\", \"MIAMI\", \"DIEGO\", \"PHOENIX\", \"SEATTLE\", \"HOUSTON\", \"SANTA\", \"ORLANDO\", \n",
    "    \"CHICAGO\", \"LAS\", \"ATLANTA\", \"LOS\", \"MESA\", \"VEGAS\", \"CHARLOTTE\", \"TAMPA\", \n",
    "    \"GREENVILLE\", \"BROOKLYN\", \"DENVER\", \"ANGELES\", \"ANTONIO\", \"MEMPHIS\", \"YORK\",\n",
    "    \"RICHMOND\", \"BEACH\", \"PALM\", \"FORT\", \"ST\", \"LAKE\", \"WEST\", \"DES\", \"PARK\",\n",
    "    \"HILL\", \"NORTH\", \"SPRING\", \"CREEK\", \"SAINT\", \"RIVER\", \"SOUTH\", \"MYERS\",\n",
    "    # Added from 1-grams\n",
    "    \"CITY\", \"NEW\", \"TROY\", \"VALLEY\", \"PORT\",\n",
    "    # Added from 2-grams\n",
    "    \"WAXAHACHIE\", \"EL CAJON\", \"PASO ROBLES\", \"BUENA VI\", \"CHULA VISTA\", \n",
    "    \"BOCA RATON\", \"PINE PLAINS\", \"HIGHLANDS RAN\", \"RENTON\", \"SALT\", \"CIT\", \"BOGOT\"\n",
    "]\n",
    "CITY_REGEX = r\"\\b(\" + \"|\".join(CITY_LIST) + r\")\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_WORDS = [\n",
    "    \"DBT\", \"PURCH\", \"TRANSACTION\", \"PMT\", \"PMTS\", \"HTTPSWWW\", \"WWW\", \"CONSUMER\", \n",
    "    \"CKCD\", \"CRD\", \"PUR\", \"PIN\", \"SIG\", \"LLC\", \"SIGNATURE\", \"WEB\", \"PAYMENT\",\n",
    "    \"ACH\", \"DEB\", \"INTL\", \"RECURRING\", \"DIGIT\", \"ONLINE\", \"PROTECTION\", \"VSA\",\n",
    "    \"800\", \"888\", \"WITHDRAWAL\", \"INDN\", \"STORE\", \"STORES\", \"RESTAURANT\", \n",
    "    \"BEAUTY\", \"DELI\",\n",
    "    \"GROCERY\", \"NAILS\", \"STOP\", \"BUSINESS\", \"PARKING\", \"PET\", \"GARDEN\", \"FIL\",\n",
    "    # Added from 1-grams (and curated)\n",
    "    \"POS\", \"PURCHASE\", \"DEBIT\", \"VENDING\", \"POINT\", \"BIG\", \"NON\", \"TR\", \"SUP\",\n",
    "    # Added from 2-grams\n",
    "    \"SNACK\", \"SODA\", \"HELP\", \"HTTPSHBOMAX\", \"HTTPSINSTACAR\", 'SUPER', 'MART', 'CENTER', 'MARKET'\n",
    "]\n",
    "NOISE_WORDS_REGEX = r\"\\b(\" + \"|\".join(NOISE_WORDS) + r\")\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_PRE = [ \n",
    "    # === 0) Normalize spaces first === \n",
    "    (r\"\\u00A0\", \" \"), # Replace non-breaking space with regular space \n",
    "    (r\"\\s{2,}\", \" \"), # Collapse multiple spaces into one \n",
    "\n",
    "    # === 1) “Authorized / Recurring” headers === \n",
    "    (r\"\\b(?:RECURRING\\s+)?PAYMENT\\s+AUTHORIZED\\s+ON(?:\\s+\\d{2}[/-]\\d{2,4})?\\b\", \" \"), \n",
    "    (r\"\\b(?:P?URCHASE\\s+)?AUTHORIZED\\s+ON(?:\\s+\\d{2}[/-]\\d{2,4})?\\b\", \" \"), \n",
    "    (r\"\\bAUTHORIZED\\s+ON\\s+\\d{2}[/-]\\d{2,4}\\b\", \" \"), \n",
    "    (r\"\\bRECURRING\\s+PYMT\\b\", \" \"), \n",
    "\n",
    "    # === 2) Card & mask boilerplate === \n",
    "    (r\"\\b(?:VISA|MASTERCARD|AMEX|DISCOVER)\\s+CHECK\\s+CARD\\b\", \" \"), \n",
    "    (r\"\\bCHECK\\s*CARD\\b(?:\\s*X+)?\", \" \"), \n",
    "    (r\"\\bCARD(?:\\s+ENDING\\s+IN)?\\s*X{4}\\b\", \" \"), \n",
    "    (r\"\\bDEBIT\\s+CARD\\s+DEBIT\\s*/\", \" \"), \n",
    "    (r\"\\b(?:DEBIT|CREDIT)\\s+CARD\\s+(?:PURCHASE|DEBIT|AUTH(?:ORIZATION)?)\\b\", \" \"), \n",
    "    (r\"\\b(?:WITHDRAWAL|POS)\\s*#\", \" \"), \n",
    "    (r\"\\bWITHDRAWAL\\s+DEBIT\\s+CHIP\\b\", \" \"), \n",
    "    (r\"\\bPOS\\s+PUR-\\s*(?:\\*+)?\", \" \"), \n",
    "    (r\"\\bAUTH\\s*#\\s*-?\", \" \"), \n",
    "    (r\"\\bCK\\s*X+\\b\", \" \"), \n",
    "    (r\"\\bPOS\\s+(?:PURCHASE|WITHDRAWAL|DEBIT)\\b\", \" \"), \n",
    "    (r\"\\b(?:DDA\\s+)?PIN\\s+POS\\s+PUR\\b\", \" \"), \n",
    "    (r\"\\bCDX{4,}\\b\", \" \"), \n",
    "    (r\"\\bF[XF]{4,}\\b\", \" \"), # Handle FXXXXX, FXXXX\n",
    "    (r\"\\bXXX\\b\", \" \"), # Handle masked number from n-grams\n",
    "    (r\"X{4,}\", \" \"), # Remove generic masked numbers \n",
    "    (r\"\\b[SP]X{6,}\\b\", \" \"), \n",
    "    (r\"\\bDEBIT\\s+(?:CARD|CRD)\\b\", \" \"), \n",
    "    (r\"\\bDEBIT\\s+PURCHASE\\b\", \" \"), \n",
    "    (r\"\\bPOS\\s+SIGNATURE\\b\", \" \"), \n",
    "    (r\"\\b(?:VISA|MASTERCARD|AMEX|DISCOVER|CARD|DATE|MCC)\\b\", \" \"), # Remove common card-related keywords \n",
    "    (r\"^\\s*PURCHASE\\b\", \" \"), # Remove \"PURCHASE\" if at start \n",
    "    (r\"^\\s*REC\\s+POS\\b\", \" \"), \n",
    "    (r\"^\\s*RECURRING\\b\", \" \"), \n",
    "\n",
    "    # === 2.5) Prefix Normalization === \n",
    "    (r\"\\b(DNH)(?=[A-Z]{2,})\", r\"\\1 \"), # Fix \"DNHGODADDYCOM\" -> \"DNH GODADDYCOM\" \n",
    "    (r\"\\bDD\\s*(?:[\\\\/]\\s*)?BR\\b\", \"DDBR\"), # Combine DD/BR or DD BR -> DDBR \n",
    "\n",
    "    # === 3) State + mask tails === \n",
    "    (r\"\\b[A-Z]{2}\\s+[SP]?X{6,}\\s+CARD\\s+X{4}\\b\", \" \"), \n",
    "    (r\"\\b[A-Z]{2}\\s+[SP]?X{6,}\\b\", \" \"), \n",
    "\n",
    "    # === 4) Dates/times === \n",
    "    (r\"\\b#?\\d{2}[/-]\\d{2}(?:[/-]\\d{2,4})?\\b\", \" \"), # Dates like 10/23, 10/23/2025 \n",
    "    (r\"\\b\\d{1,2}\\s+\\d{2}\\s+\\d{2}\\s*(?:AM|PM)\\b\", \" \"), # 10 23 25 PM \n",
    "    (r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM)\\b\", \" \"), # Times like 10:23 AM \n",
    "\n",
    "    # === 4.5) Noise numbers (from n-grams) ===\n",
    "    (r\"\\b(?:00|10|15|16|20|365)\\b\", \" \"),\n",
    "\n",
    "    # === 5) Merchant-terminal boilerplate === \n",
    "    (r\"\\bMERCHANT\\s+PURCHASE\\s+TERMINAL\\b\\s*-?\", \" \"), \n",
    "    (r\"\\bPOINT\\s+OF\\s+SALE\\s+(?:WITHDRAWAL|DEBIT)\\b\\s*-?\", \" \"), \n",
    "    (r\"\\b(?:CRD|ACH)\\s+TRAN(?:\\s+PPD(?:\\s+ID)?)?\\b\", \" \"), \n",
    "    (r\"\\bCO\\s+ID\\s+\\w+\\s+(?:WEB|PPD)\\b.*\", \" \"), # Remove CO ID... \n",
    "     \n",
    "    # === 6) Misc tails === \n",
    "    (r\"\\b(?:INST|PAYPAL)\\s+XFER\\b\", \" \"), \n",
    "    (r\"\\b(?:XFER|WEB)\\s+ID\\b.*\", \" \"), \n",
    "    (r\"\\b(?:ELECTRONIC|EXTERNAL)\\s+WITHDRAWAL\\b\", \" \"), \n",
    "    (r\"\\bWITHDRAWAL\\s+DEBIT\\s+CARD\\b(?:\\s+DEBIT)?\", \" \"), \n",
    "    (r\"\\bO(?:F)?\\s+SALE\\s+DEBIT\\s+L\\d{3}\\b.*\", \" \"), \n",
    "    (r\"\\b(?:ITEM|OVERDRAFT)\\s+FEE\\s+FOR\\s+ACTIVITY\\b.*\", \" \"), \n",
    "    (r\"\\b(?:GENESIS[-\\s]*FS\\s+CARD\\s+PAYMENT)\\b\", \" \"), \n",
    "    (r\"\\bBILL\\s+PAYMENT\\b\", \" \"), \n",
    "    (r\"\\b(?:US|WA)\\s+CARD\\s+PURCHASE\\b\", \" \"), \n",
    "    (r\"-\\s*MEMO=\", \" \"), \n",
    "    (r\"(?:USA|US)$\", \" \"), # Remove USA or US at the end \n",
    "    (r\"\\s+FSP$\", \" \"), \n",
    "    (r\"\\bL\\d{3}\\b\", \" \"), # Handle L340\n",
    "\n",
    "    # === 7) Phone numbers === \n",
    "    (r\"\\b(?:\\d{3}-\\d{3}-\\d{4}|XXX-XXX-XXXX)\\b\", \" \"), # 800-555-1212 \n",
    "    (r\"\\b\\d{3}-\\d{4}\\b\", \" \"), # 555-1212 \n",
    "    (r\"\\b(?:\\d{3}\\s*){1,2}\\d{3}\\s*\\d{3,4}\\b\", \" \"), # 800 555 1212 or 1 800 555 1212 \n",
    "    (r\"\\b#?\\s*\\d{3}-\\d{3}-\\d{1,4}\\s*(?:AM|PM)?\\b\", \" \"), \n",
    "\n",
    "    # === 8) URLs/domains === \n",
    "    (r\"^\\.COM\\s+BILL\\b.*\", \" \"), \n",
    "    (r\"\\.COM\\b\", \" \"), # Remove .COM anywhere\n",
    "    (r\"\\s+COM$\", \" \"), # Remove .COM at end of string\n",
    "\n",
    "    # === 9) State abbreviations === \n",
    "    (STATE_REGEX, \" \"), # Remove standalone state codes \n",
    "    \n",
    "    # === 9.5) City abbreviations ===\n",
    "    (CITY_REGEX, \" \"), # Remove standalone city names\n",
    "    \n",
    "    # === 10) Noise Words (from 1-grams) ===\n",
    "    (NOISE_WORDS_REGEX, \" \"),\n",
    "\n",
    "    # === 11) Final Tidy (Punctuation) === \n",
    "    (r\"[|%_=;\\\\/]+\", \" \"), # Remove misc separators \n",
    "    (r\"[-]{2,}\", \" \"), # Collapse multiple hyphens\n",
    "    (r\"^\\s*-\\s*|\\s*-\\s*$\", \" \"), # Remove leading/trailing hyphens\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_POST = [\n",
    "    # --- Specific/Tricky Rules ---\n",
    "    # This greedily finds the *last* instance of GODADDY.COM or GODADDY\n",
    "    re.compile(r\".*(GODADDY\\.COM|GODADDY)\\b.*\"),\n",
    "    \n",
    "    # Rules for 'PAYPAL [MERCHANT] INTERNET PAYMENT' format\n",
    "    re.compile(r\"^PAYPAL\\s+([A-Z\\s0-9'.*-]+?)\\s+(?:INTERNET\\s+PAYMENT|COID.*)?.*$\"),\n",
    "\n",
    "    # Rules for '[MERCHANT] ... PAYPAL' format\n",
    "    re.compile(r\"^([A-Z\\s0-9'.*-]+?)\\s+PAYPAL.*$\"),\n",
    "\n",
    "    # === NEW PAYPAL RULES ===\n",
    "    # Handle 'PAYPAL *... MERCHANT' (from INST XFER)\n",
    "    re.compile(r\"^PAYPAL\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    # Handle 'PAYPAL : MERCHANT' (from INST XFER / ID:)\n",
    "    re.compile(r\"^PAYPAL\\s*:\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "\n",
    "    # === NEW: High-Frequency Full Merchant Names (from 1-grams) ===\n",
    "    re.compile(r\"^(AFTERPAY)\\b.*\"),\n",
    "    re.compile(r\"^(ALDI)\\b.*\"),\n",
    "    re.compile(r\"^(AMZN(?:\\s*MKTP)?)\\b.*\"), # Handles amzn, amzn mktp\n",
    "    re.compile(r\"^(AMAZON(?:\\.COM|\\s+PRIME)?)\\b.*\"), # Handles amazon, amazon prime\n",
    "    re.compile(r\"^(APPLE(?:\\.COM)?)\\b.*\"),\n",
    "    re.compile(r\"^(BRIGIT)\\b.*\"),\n",
    "    re.compile(r\"^(BURGER\\s+KING)\\b.*\"),\n",
    "    re.compile(r\"^(CASH\\s+APP)\\b.*\"),\n",
    "    re.compile(r\"^(CHICK-FIL-A)\\b.*\"),\n",
    "    re.compile(r\"^(CHIPOTLE)\\b.*\"),\n",
    "    re.compile(r\"^(CIRCLE\\s+K)\\b.*\"),\n",
    "    re.compile(r\"^(COSTCO)\\b.*\"),\n",
    "    re.compile(r\"^(DAIRY\\s+QUEEN)\\b.*\"),\n",
    "    re.compile(r\"^(DOLLAR\\s+GENERAL)\\b.*\"),\n",
    "    re.compile(r\"^(DOLLAR\\s+TREE)\\b.*\"),\n",
    "    re.compile(r\"^(DOORDASH)\\b.*\"),\n",
    "    re.compile(r\"^(DUNKIN)\\b.*\"),\n",
    "    re.compile(r\"^(EBAY)\\b.*\"), # Added from n-grams\n",
    "    re.compile(r\"^(ETSY)\\b.*\"),\n",
    "    re.compile(r\"^(FAMILY\\s+DOLLAR)\\b.*\"),\n",
    "    re.compile(r\"^(FOOD\\s+LION)\\b.*\"),\n",
    "    re.compile(r\"^(FRYS)\\b.*\"),\n",
    "    re.compile(r\"^(GOOGLE)\\b.*\"),\n",
    "    re.compile(r\"^(HELPPAY)\\b.*\"),\n",
    "    re.compile(r\"^(HOME\\s+DEPOT)\\b.*\"),\n",
    "    re.compile(r\"^(INSTACART)\\b.*\"),\n",
    "    re.compile(r\"^(KFC)\\b.*\"),\n",
    "    re.compile(r\"^(KROGER)\\b.*\"),\n",
    "    re.compile(r\"^(LITTLE\\s+CAESARS)\\b.*\"),\n",
    "    re.compile(r\"^(LOWE'?S)\\b.*\"),\n",
    "    re.compile(r\"^(LYFT)\\b.*\"),\n",
    "    re.compile(r\"^(MCDONALD'?S)\\b.*\"),\n",
    "    re.compile(r\"^(MICROSOFT)\\b.*\"),\n",
    "    re.compile(r\"^(PUBLIX)\\b.*\"),\n",
    "    re.compile(r\"^(ROSS)\\b.*\"),\n",
    "    re.compile(r\"^(SAFEWAY)\\b.*\"),\n",
    "    re.compile(r\"^(SAMS\\s*CLUB|SAMSCLUB)\\b.*\"), # Updated from n-grams\n",
    "    re.compile(r\"^(SHOPRITE)\\b.*\"),\n",
    "    re.compile(r\"^(SONIC)\\b.*\"),\n",
    "    re.compile(r\"^(STARBUCKS)\\b.*\"),\n",
    "    re.compile(r\"^(SUBWAY)\\b.*\"),\n",
    "    re.compile(r\"^(TACO\\s+BELL)\\b.*\"),\n",
    "    re.compile(r\"^(TARGET)\\b.*\"),\n",
    "    re.compile(r\"^(UBER(?:\\s+EATS)?)\\b.*\"),\n",
    "    re.compile(r\"^(USPS)\\b.*\"),\n",
    "    re.compile(r\"^(VONS)\\b.*\"),\n",
    "    # Updated from n-grams (wal, mart, wm, superc, supercenter)\n",
    "    re.compile(r\"^(WALMART(?:\\s*SUPERCENTER|\\s*SUPER\\s*C)?|WM\\s*SUPERCENTER|WAL-MART|WAL\\s*MART)\\b.*\"),\n",
    "    re.compile(r\"^(WENDY'?S)\\b.*\"),\n",
    "\n",
    "    # --- Standardized Prefix Rules ---\n",
    "    # (Capture group is placed *after* the prefix)\n",
    "    re.compile(r\"^ACI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^KING\\s*#\\s*([A-Z\\s09'.-]+).*\"),\n",
    "    re.compile(r\"^ACE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    \n",
    "    # === 7-ELEVEN RULES ===\n",
    "    re.compile(r\"^(7(?:-ELEVEN|\\s+11))\\s*\\*?#?.*\"),\n",
    "    \n",
    "    re.compile(r\"^ZG\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^YSI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "\n",
    "    # === UPDATED RULE ORDER ===\n",
    "    re.compile(r\"^(DDBR)\\s*\\*?#?.*\"), \n",
    "    \n",
    "    re.compile(r\"^DD\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CCI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PY\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ANC\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^J2\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^OSP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PL\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^RTI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^FSP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PT\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CHR\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^USA\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^SP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    \n",
    "    # NEW: Rule for HOME* AHS.COM pattern\n",
    "    re.compile(r\"^HOME\\s*\\*?\\s*([A-Z\\s0-9'.-]+?)(?:\\s+[A-Z]{2,})?.*\"),\n",
    "    \n",
    "    # === TST RULES ===\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s09'.-]+?)\\s+[A-Z]{2,}\\s+[A-Z]{2,}\\s+ON\\s*##.*\"),\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s09'.-]+?)\\s+[A-Z]{2,}\\s+ON\\s*##.*\"),\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"), \n",
    "    \n",
    "    re.compile(r\"^CKE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^SQ\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^SP\\+AFF\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^IC\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"), \n",
    "    re.compile(r\"^SIE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    \n",
    "    # Generic PAYPAL rule, catches PAYPAL*MERCHANT\n",
    "    re.compile(r\"^PAYPAL\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"), \n",
    "\n",
    "    # --- Specific '*' prefix rules (already correct) ---\n",
    "    re.compile(r\"^AMS\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZSK\\s*\\*+ZSK\\s*\\*+([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZSK\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CCM\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZIP\\.CO\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^XSOLLA\\s*\\*+\\s*([A-Z\\s0EXAMPLE'.-]+).*\"),\n",
    "    re.compile(r\"^NOR\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "\n",
    "    \n",
    "    # --- Generic Fallback Rules ---\n",
    "    # These rules try to find the merchant *before* a common delimiter.\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME * JUNK\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s*\\*.*\"),\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME # JUNK\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s*#.*\"),\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME - Exp\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s+-\\s+EXP.*\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# First pass\n",
    "memos = df['memo'].astype(str).fillna('').str.upper()\n",
    "memos = memos.str.replace(r\"\\u00A0\", \" \", regex=True)\n",
    "memos = memos.str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "memos = memos.str.strip()\n",
    "\n",
    "for pattern, repl in REGEX_PRE:\n",
    "    memos = memos.str.replace(pattern, repl, regex=True)\n",
    "\n",
    "memos = memos.str.replace(NOISE_WORDS_REGEX, \" \", regex=True)\n",
    "memos = memos.str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "memos = memos.str.replace(r\"^[\\s-]+|[\\s-]+$\", \"\", regex=True)\n",
    "df['memo_pre'] = memos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Second pass\n",
    "def apply_regex(memo):\n",
    "    for pattern in REGEX_POST:\n",
    "        match = pattern.match(memo)\n",
    "        if match:\n",
    "            # Use the last non-null group if multiple are present\n",
    "            # (handles the GODADDY rule)\n",
    "            groups = match.groups()\n",
    "            if groups:\n",
    "                return groups[-1].strip()\n",
    "    return memo\n",
    "\n",
    "df['memo_post'] = df['memo_pre'].apply(apply_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('memos_P1.csv', index=False) # Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manually check\n",
    "# unique_df = df.drop_duplicates(subset='memo_post')\n",
    "# result = (\n",
    "#     unique_df[unique_df['memo_pre'].str.split().str.len() == 1]\n",
    "#     .sort_values(by='memo_pre')[100:200].to_string()\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10).sort_values(by='memo_post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants_clean = ['AMAZON', 'ALBERTSONS', 'ADOBE', 'SONIC', 'LIDL', 'AFTERPAY', 'ARBYS', 'ALDI', 'AUDIBLE', 'DOLLARTREE', 'LOWES', 'KROGER', 'DUNKIN', 'HP', 'PUBLIX', 'FRYS', 'SAFEWAY', 'DOORDASH', 'GOOGLE', 'WALMART', 'CHICK-FIL-A', 'SAMSCLUB', 'MICROSOFT',\n",
    "             'UBER', 'ULTA', 'H-E-B', 'VONS', 'CMSVEND', 'INSTACART', 'LYFT', 'TJMAXX', 'PETSMART', 'THORNTONS', 'PAYPAL', 'MAVERIK', 'WENDYS', 'MARSHALLS', 'ALLSUPS', 'SUNPASS', 'QVC', 'PRIZEPICKS', 'DDBR']\n",
    "\n",
    "merchants_punc = [\"DENNY'S\", 'WAL-MART', \"LOWE'S\", 'DOLLAR-GENERAL', '7-ELEVEN', \"WENDY'S\", \"ZAXBY'S\", 'FRYS-FOOD-DRG', \"BUC-EE'S\",\n",
    "                 \"BASHAS'\"]\n",
    "\n",
    "merchants_sites = ['AMAZON.COM', 'GODADDY.COM', 'CCBILL.COM']\n",
    "\n",
    "merchant_cats = ['', 'OVERDRAFT', 'WITHDRAWAL']\n",
    "\n",
    "multiples = [['AMAZON', 'AMAZON.COM', 'AMAZON PRIME'], ['LOWES', \"LOWE'S\"], ['BASKIN', 'DDBR']]\n",
    "\n",
    "merchants = merchants_clean + merchants_punc + merchants_sites + merchant_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for memo in df[df['memo_post'].str.split().str.len() == 1]['memo_post'].value_counts().sort_values(ascending=False).index:\n",
    "    if memo not in merchants:\n",
    "        print(memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['memo_post'] == '']#.iloc[0]['memo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['memo_post'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = r\"^[A-Z0-9\\.]+\\s*\\*\\s*[A-Z\\s0-9'.-]+\"\n",
    "\n",
    "prefix_star_merchants = df[df['memo_pre'].str.contains(regex_pattern, na=False)]\n",
    "prefix_star_merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(prefix_star_merchants['memo_pre'].str.split('*').sample(100).sort_values().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Extract & Analyze N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[df['memo_post'].str.len() < 4]['memo_post'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(corpus: pd.Series, n_gram_range: tuple, top_n: int = 200):\n",
    "    print(f\"Analyzing {n_gram_range} n-grams...\")\n",
    "    vec = CountVectorizer(\n",
    "        ngram_range=n_gram_range,\n",
    "        stop_words='english',\n",
    "        max_features=None  # We want to count all n-grams first\n",
    "    ).fit(corpus)\n",
    "    \n",
    "    # Get the counts\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    # Sum the counts for each n-gram\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    \n",
    "    # Map n-grams to their frequencies\n",
    "    words_freq = [\n",
    "        (word, sum_words[0, idx]) \n",
    "        for word, idx in vec.vocabulary_.items()\n",
    "    ]\n",
    "    \n",
    "    # Sort by frequency (descending)\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = df['memo_post'].fillna('')\n",
    "print(f\"Analyzing {len(corpus)} cleaned memos...\")\n",
    "# Get the top 200 of each n-gram type\n",
    "top_1grams = top_ngrams(corpus, n_gram_range=(1, 1), top_n=100)\n",
    "top_2grams = top_ngrams(corpus, n_gram_range=(2, 2), top_n=100)\n",
    "top_3grams = top_ngrams(corpus, n_gram_range=(3, 3), top_n=100)\n",
    "print(f\"--- N-gram Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_1grams\n",
    "ngrams_1 = []\n",
    "for ngram, value in top_1grams:\n",
    "    if ngram.upper() not in NOISE_WORDS:\n",
    "        ngrams_1 += [ngram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_1grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_2 = [ngram for ngram, count in top_2grams]\n",
    "ngrams_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1 grams to find prefixes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
