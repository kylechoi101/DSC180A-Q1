{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Preprocessing with Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['git', 'pull'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run(['git', 'pull'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 9b94137] regex\n",
      " 1 file changed, 14357 insertions(+), 759 deletions(-)\n",
      " rewrite Week 2/Haris_Saif.ipynb (66%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['git', 'commit', '-m', 'regex'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['git', 'add', 'Haris_Saif.ipynb'])\n",
    "subprocess.run(['git', 'commit', '-m', 'regex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528766"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in = pd.read_csv(\"memos.csv\")\n",
    "df = df_in#.sample(100_000).copy()\n",
    "row_count = df.size\n",
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     156 BRAUMS STORE DDA PIN POS PUR CDXXXX OWASSO...\n",
       "1                  AMAZON.COM*1O1A AMZN.COM/BILL WA USA\n",
       "2     CHECKCARD XXXX WALMART.COM AA XXX-XXX-XXXX AR ...\n",
       "3     Debit Card PHNXCHLDRNCAFE XXXX E THOMAS ROAD P...\n",
       "4     Debit Purchase 10/07 Card XXXXwm Superc Wal-ma...\n",
       "5                                         Donatos Pizza\n",
       "6                                      Habit Missionval\n",
       "7                         KFC GXXXXXX SARASOTA FL 08/26\n",
       "8     Mission Lane Vis Mission La ST-R8J1D5Q7P7A7 LI...\n",
       "9     POS Debit - Visa Check Card XXXX - MISSION BBQ...\n",
       "10    POS PURCHASE / MERCHANT PURCHASE TERMINAL XXXX...\n",
       "11               POS Withdrawal MARINA POKE / HOUMarina\n",
       "12    PURCHASE AUTHORIZED ON 03/02 Phantom EF g.co/h...\n",
       "13    PURCHASE AUTHORIZED ON 03/16 SHOPRITE JACKSON ...\n",
       "14    PURCHASE AUTHORIZED ON 04/10 CENTER MARKET 33 ...\n",
       "15    PURCHASE AUTHORIZED ON 05/31 ERC-COMCAST XXX-X...\n",
       "16    PURCHASE AUTHORIZED ON 06/25 TRACTOR-SUPPLY-CO...\n",
       "17    PURCHASE AUTHORIZED ON 07/25 PEET'S #XXXXX VEN...\n",
       "18    PURCHASE AUTHORIZED ON 09/05 SMITHS FO XXXX HW...\n",
       "19    PURCHASE AUTHORIZED ON 09/14 WINCO FOODS #53 2...\n",
       "20    PURCHASE AUTHORIZED ON 12/22 DUNKIN #XXXXXX Q3...\n",
       "21           Point Of Sale Withdrawal MINUTE MARKET #10\n",
       "22                                 Shopbop.com WI 12/15\n",
       "23                               Sparkies Kitchen & Bar\n",
       "24                               The Old Village Trader\n",
       "Name: memo, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['memo'].sample(25).sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Regex Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_LIST = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \n",
    "    \"(?<!\\.)CO(?!['`])\", # Negative lookbehind/ahead for CO (e.g., not .CO or COSTCO)\n",
    "    \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \"HI\", \"IA\", \n",
    "    \"ID\", \"IL\", \n",
    "    \"IN(?!\\\\s+N\\\\s+OUT\\\\s+BURGER)\", # Negative lookahead for IN (not IN N OUT BURGER)\n",
    "    \"KS\", \"KY\",\n",
    "    \"(?<!['`])LA(?!\\\\s+HACIENDA|\\\\s+FITNESS|\\\\s+LA'S|['`])\", # Negative lookaheads for LA\n",
    "    \"MA\", \"MD\", \n",
    "    \"ME(?!\\\\s+DIA)\", # Negative lookahead for ME (not ME DIA)\n",
    "    \"MI\", \"MN\", \"MO(?!['`])\", \n",
    "    \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \n",
    "    \"OH\", \"OK\", \"OR\", \n",
    "    \"PA(?!['`])\", \n",
    "    \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \n",
    "    \"WI\", \"WV\", \"WY\"\n",
    "]\n",
    "STATE_REGEX = r\"\\b(\" + \"|\".join(STATE_LIST) + r\")\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From 1-gram\n",
    "NOISE_WORDS = [\n",
    "    \"DBT\", \"PURCH\", \"TRANSACTION\", \"PMT\", \"PMTS\", \"HTTPSWWW\", \"WWW\", \"CONSUMER\", \n",
    "    \"CKCD\"\n",
    "]\n",
    "NOISE_WORDS_REGEX = r\"\\b(\" + \"|\".join(NOISE_WORDS) + r\")\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_PRE = [\n",
    "    # === 0) Normalize spaces first ===\n",
    "    (r\"\\u00A0\", \" \"), # Replace non-breaking space with regular space\n",
    "    (r\"\\s{2,}\", \" \"), # Collapse multiple spaces into one\n",
    "\n",
    "    # === 1) “Authorized / Recurring” headers ===\n",
    "    (r\"\\b(?:RECURRING\\s+)?PAYMENT\\s+AUTHORIZED\\s+ON(?:\\s+\\d{2}[/-]\\d{2,4})?\\b\", \" \"),\n",
    "    (r\"\\b(?:P?URCHASE\\s+)?AUTHORIZED\\s+ON(?:\\s+\\d{2}[/-]\\d{2,4})?\\b\", \" \"),\n",
    "    (r\"\\bAUTHORIZED\\s+ON\\s+\\d{2}[/-]\\d{2,4}\\b\", \" \"),\n",
    "    (r\"\\bRECURRING\\s+PYMT\\b\", \" \"),\n",
    "\n",
    "    # === 2) Card & mask boilerplate ===\n",
    "    (r\"\\b(?:VISA|MASTERCARD|AMEX|DISCOVER)\\s+CHECK\\s+CARD\\b\", \" \"),\n",
    "    (r\"\\bCHECK\\s*CARD\\b(?:\\s*X+)?\", \" \"), \n",
    "    (r\"\\bCARD(?:\\s+ENDING\\s+IN)?\\s*X{4}\\b\", \" \"),\n",
    "    (r\"\\bDEBIT\\s+CARD\\s+DEBIT\\s*/\", \" \"),\n",
    "    (r\"\\b(?:DEBIT|CREDIT)\\s+CARD\\s+(?:PURCHASE|DEBIT|AUTH(?:ORIZATION)?)\\b\", \" \"),\n",
    "    (r\"\\b(?:WITHDRAWAL|POS)\\s*#\", \" \"), \n",
    "    (r\"\\bWITHDRAWAL\\s+DEBIT\\s+CHIP\\b\", \" \"),\n",
    "    (r\"\\bPOS\\s+PUR-\\s*(?:\\*+)?\", \" \"), \n",
    "    (r\"\\bAUTH\\s*#\\s*-?\", \" \"), \n",
    "    (r\"\\bCK\\s*X+\\b\", \" \"),\n",
    "    (r\"\\bPOS\\s+(?:PURCHASE|WITHDRAWAL|DEBIT)\\b\", \" \"), \n",
    "    (r\"\\b(?:DDA\\s+)?PIN\\s+POS\\s+PUR\\b\", \" \"), \n",
    "    (r\"\\bCDX{4,}\\b\", \" \"),\n",
    "    (r\"\\bX{4,}\\b\", \" \"), # PRECISE: Remove standalone masked numbers\n",
    "    (r\"\\b[SP]X{6,}\\b\", \" \"), \n",
    "    (r\"\\bDEBIT\\s+(?:CARD|CRD)\\b\", \" \"), \n",
    "    (r\"\\bDEBIT\\s+PURCHASE\\b\", \" \"), \n",
    "    (r\"\\bPOS\\s+SIGNATURE\\b\", \" \"),\n",
    "    (r\"\\b(?:VISA|MASTERCARD|AMEX|DISCOVER|CARD|DATE|MCC)\\b\", \" \"), # Remove common card-related keywords\n",
    "    (r\"^\\s*PURCHASE\\b\", \" \"), # Remove \"PURCHASE\" if at start\n",
    "    (r\"^\\s*REC\\s+POS\\b\", \" \"),\n",
    "    (r\"^\\s*RECURRING\\b\", \" \"),\n",
    "\n",
    "    # === 2.5) Prefix Normalization ===\n",
    "    (r\"\\b(DNH)(?=[A-Z]{2,})\", r\"\\1 \"), # Fix \"DNHGODADDYCOM\" -> \"DNH GODADDYCOM\"\n",
    "    (r\"\\bDD\\s*(?:[\\\\/]\\s*)?BR\\b\", \"DDBR\"), # Combine DD/BR or DD BR -> DDBR\n",
    "\n",
    "    # === 3) State + mask tails ===\n",
    "    (r\"\\b[A-Z]{2}\\s+[SP]?X{6,}\\s+CARD\\s+X{4}\\b\", \" \"),\n",
    "    (r\"\\b[A-Z]{2}\\s+[SP]?X{6,}\\b\", \" \"),\n",
    "\n",
    "    # === 4) Dates/times (INCLUSIVE) ===\n",
    "    (r\"\\b#?\\d{2}[/-]\\d{2}(?:[/-]\\d{2,4})?\\b\", \" \"), # Dates like 10/23, 10/23/2025\n",
    "    (r\"\\b\\d{4}-\\d{2}-\\d{2}\\b\", \" \"), # YYYY-MM-DD\n",
    "    (r\"\\b\\d{1,2}-(?:JAN|FEB|MAR|APR|MAY|JUN|JUL|AUG|SEP|OCT|NOV|DEC)-\\d{2,4}\\b\", \" \"), # DD-MMM-YYYY\n",
    "    (r\"\\b(?:19|20)\\d{2}(?:0[1-9]|1[0-2])(?:0[1-9]|[12]\\d|3[01])\\b\", \" \"), # YYYYMMDD\n",
    "    (r\"\\b(?:0[1-9]|1[0-2])(?:0[1-9]|[12]\\d|3[01])\\d{2}\\b\", \" \"), # MMDDYY\n",
    "    (r\"\\b\\d{1,2}\\s+\\d{2}\\s+\\d{2}\\s*(?:AM|PM)\\b\", \" \"), # 10 23 25 PM\n",
    "    (r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM)\\b\", \" \"), # Times like 10:23 AM\n",
    "\n",
    "    # === 5) Merchant-terminal boilerplate ===\n",
    "    (r\"\\bMERCHANT\\s+PURCHASE\\s+TERMINAL\\b\\s*-?\", \" \"),\n",
    "    (r\"\\bPOINT\\s+OF\\s+SALE\\s+(?:WITHDRAWAL|DEBIT)\\b\\s*-?\", \" \"),\n",
    "    (r\"\\b(?:CRD|ACH)\\s+TRAN(?:\\s+PPD(?:\\s+ID)?)?\\b\", \" \"),\n",
    "    (r\"\\bCO\\s+ID\\s+\\w+\\s+(?:WEB|PPD)\\b.*\", \" \"), # Remove CO ID...\n",
    "    \n",
    "    # === 6) Misc tails (INCLUSIVE) ===\n",
    "    (r\"\\b(?:INST|PAYPAL)\\s+XFER\\b\", \" \"), \n",
    "    (r\"\\b(?:XFER|WEB)\\s+ID\\b.*\", \" \"),\n",
    "    (r\"\\bID\\b\", \" \"), # Remove standalone 'ID' (from 'ID: DSW')\n",
    "    (r\"\\b(?:REF|TERM|TRN|INV|ACCT|TID|MID)\\s*#?[\\d\\w-]+\\b\", \" \"), # REF 123, TERM 001, etc.\n",
    "    (r\"\\bAUTH\\s+CODE\\s*[\\d\\w-]+\\b\", \" \"), # AUTH CODE 0123\n",
    "    (r\"\\b(?:ELECTRONIC|EXTERNAL)\\s+WITHDRAWAL\\b\", \" \"), \n",
    "    (r\"\\bWITHDRAWAL\\s+DEBIT\\s+CARD\\b(?:\\s+DEBIT)?\", \" \"),\n",
    "    (r\"\\bO(?:F)?\\s+SALE\\s+DEBIT\\s+L\\d{3}\\b.*\", \" \"),\n",
    "    (r\"\\b(?:ITEM|OVERDRAFT)\\s+FEE\\s+FOR\\s+ACTIVITY\\b.*\", \" \"),\n",
    "    (r\"\\b(?:GENESIS[-\\s]*FS\\s+CARD\\s+PAYMENT)\\b\", \" \"),\n",
    "    (r\"\\bBILL\\s+PAYMENT\\b\", \" \"),\n",
    "    (r\"\\b(?:US|WA)\\s+CARD\\s+PURCHASE\\b\", \" \"),\n",
    "    (r\"-\\s*MEMO=\", \" \"),\n",
    "    (r\"(?:USA|US)$\", \" \"), # Remove USA or US at the end\n",
    "    (r\"\\s+FSP$\", \" \"),\n",
    "\n",
    "    # === 7) Phone numbers (INCLUSIVE) ===\n",
    "    (r\"\\b1[\\s.-]\\(?\\d{3}\\)?[\\s.-]\\d{3}[\\s.-]\\d{4}\\b\", \" \"), # 1-800-555-1212\n",
    "    (r\"\\b\\(?\\d{3}\\)?[\\s.-]\\d{3}[\\s.-]\\d{4}\\b\", \" \"), # (800) 555-1212, 800.555.1212\n",
    "    (r\"\\b\\d{3}-\\d{4}\\b\", \" \"), # 555-1212\n",
    "    (r\"\\bXXX-XXX-XXXX\\b\", \" \"), # Masked phone\n",
    "\n",
    "    # === 8) URLs/domains (INCLUSIVE) ===\n",
    "    (r\"^\\.COM\\s+BILL\\b.*\", \" \"),\n",
    "    (r\"\\s+\\.(?:COM|NET|ORG|GOV|EDU|IO|CO)\\b\", \" \"), # Remove trailing .COM, .NET etc.\n",
    "\n",
    "    # === 9) State abbreviations ===\n",
    "    (STATE_REGEX, \" \"), # Remove standalone state codes\n",
    "\n",
    "    # === 10) Final Tidy (Punctuation) ===\n",
    "    (r\"[|%_=;\\\\/]+\", \" \"), # Remove misc separators\n",
    "    (r\"[-]{2,}\", \" \"), # Collapse multiple hyphens\n",
    "    (r\"^\\s*-\\s+\", \" \") # Remove leading hyphens\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_PRE = [\n",
    "    # === 0) Normalize spaces first ===\n",
    "    (r\"\\u00A0\", \" \"), # Replace non-breaking space with regular space\n",
    "    (r\"\\s{2,}\", \" \"), # Collapse multiple spaces into one\n",
    "\n",
    "    # === 1) “Authorized / Recurring” headers ===\n",
    "    (r\"\\b(?:RECURRING\\s+)?PAYMENT\\s+AUTHORIZED\\s+ON(?:\\s+\\d{2}[/-]\\d{2,4})?\\b\", \" \"),\n",
    "    (r\"\\b(?:P?URCHASE\\s+)?AUTHORIZED\\s+ON(?:\\s+\\d{2}[/-]\\d{2,4})?\\b\", \" \"),\n",
    "    (r\"\\bAUTHORIZED\\s+ON\\s+\\d{2}[/-]\\d{2,4}\\b\", \" \"),\n",
    "    (r\"\\bRECURRING\\s+PYMT\\b\", \" \"),\n",
    "\n",
    "    # === 2) Card & mask boilerplate ===\n",
    "    (r\"\\b(?:VISA|MASTERCARD|AMEX|DISCOVER)\\s+CHECK\\s+CARD\\b\", \" \"),\n",
    "    (r\"\\bCHECK\\s*CARD\\b(?:\\s*X+)?\", \" \"), \n",
    "    (r\"\\bCARD(?:\\s+ENDING\\s+IN)?\\s*X{4}\\b\", \" \"),\n",
    "    (r\"\\bDEBIT\\s+CARD\\s+DEBIT\\s*/\", \" \"),\n",
    "    (r\"\\b(?:DEBIT|CREDIT)\\s+CARD\\s+(?:PURCHASE|DEBIT|AUTH(?:ORIZATION)?)\\b\", \" \"),\n",
    "    (r\"\\b(?:WITHDRAWAL|POS)\\s*#\", \" \"), \n",
    "    (r\"\\bWITHDRAWAL\\s+DEBIT\\s+CHIP\\b\", \" \"),\n",
    "    (r\"\\bPOS\\s+PUR-\\s*(?:\\*+)?\", \" \"), \n",
    "    (r\"\\bAUTH\\s*#\\s*-?\", \" \"), \n",
    "    (r\"\\bCK\\s*X+\\b\", \" \"),\n",
    "    (r\"\\bPOS\\s+(?:PURCHASE|WITHDRAWAL|DEBIT)\\b\", \" \"), \n",
    "    (r\"\\b(?:DDA\\s+)?PIN\\s+POS\\s+PUR\\b\", \" \"), \n",
    "    (r\"\\bCDX{4,}\\b\", \" \"),\n",
    "    (r\"X{4,}\", \" \"), # Remove generic masked numbers\n",
    "    (r\"\\b[SP]X{6,}\\b\", \" \"), \n",
    "    (r\"\\bDEBIT\\s+(?:CARD|CRD)\\b\", \" \"), \n",
    "    (r\"\\bDEBIT\\s+PURCHASE\\b\", \" \"), \n",
    "    (r\"\\bPOS\\s+SIGNATURE\\b\", \" \"),\n",
    "    (r\"\\b(?:VISA|MASTERCARD|AMEX|DISCOVER|CARD|DATE|MCC)\\b\", \" \"), # Remove common card-related keywords\n",
    "    (r\"^\\s*PURCHASE\\b\", \" \"), # Remove \"PURCHASE\" if at start\n",
    "    (r\"^\\s*REC\\s+POS\\b\", \" \"),\n",
    "    (r\"^\\s*RECURRING\\b\", \" \"),\n",
    "\n",
    "    # === 2.5) Prefix Normalization ===\n",
    "    (r\"\\b(DNH)(?=[A-Z]{2,})\", r\"\\1 \"), # Fix \"DNHGODADDYCOM\" -> \"DNH GODADDYCOM\"\n",
    "    (r\"\\bDD\\s*(?:[\\\\/]\\s*)?BR\\b\", \"DDBR\"), # Combine DD/BR or DD BR -> DDBR\n",
    "\n",
    "    # === 3) State + mask tails ===\n",
    "    (r\"\\b[A-Z]{2}\\s+[SP]?X{6,}\\s+CARD\\s+X{4}\\b\", \" \"),\n",
    "    (r\"\\b[A-Z]{2}\\s+[SP]?X{6,}\\b\", \" \"),\n",
    "\n",
    "    # === 4) Dates/times ===\n",
    "    (r\"\\b#?\\d{2}[/-]\\d{2}(?:[/-]\\d{2,4})?\\b\", \" \"), # Dates like 10/23, 10/23/2025\n",
    "    (r\"\\b\\d{1,2}\\s+\\d{2}\\s+\\d{2}\\s*(?:AM|PM)\\b\", \" \"), # 10 23 25 PM\n",
    "    (r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM)\\b\", \" \"), # Times like 10:23 AM\n",
    "\n",
    "    # === 5) Merchant-terminal boilerplate ===\n",
    "    (r\"\\bMERCHANT\\s+PURCHASE\\s+TERMINAL\\b\\s*-?\", \" \"),\n",
    "    (r\"\\bPOINT\\s+OF\\s+SALE\\s+(?:WITHDRAWAL|DEBIT)\\b\\s*-?\", \" \"),\n",
    "    (r\"\\b(?:CRD|ACH)\\s+TRAN(?:\\s+PPD(?:\\s+ID)?)?\\b\", \" \"),\n",
    "    (r\"\\bCO\\s+ID\\s+\\w+\\s+(?:WEB|PPD)\\b.*\", \" \"), # Remove CO ID...\n",
    "    \n",
    "    # === 6) Misc tails ===\n",
    "    (r\"\\b(?:INST|PAYPAL)\\s+XFER\\b\", \" \"), \n",
    "    (r\"\\b(?:XFER|WEB)\\s+ID\\b.*\", \" \"),\n",
    "    (r\"\\b(?:ELECTRONIC|EXTERNAL)\\s+WITHDRAWAL\\b\", \" \"), \n",
    "    (r\"\\bWITHDRAWAL\\s+DEBIT\\s+CARD\\b(?:\\s+DEBIT)?\", \" \"),\n",
    "    (r\"\\bO(?:F)?\\s+SALE\\s+DEBIT\\s+L\\d{3}\\b.*\", \" \"),\n",
    "    (r\"\\b(?:ITEM|OVERDRAFT)\\s+FEE\\s+FOR\\s+ACTIVITY\\b.*\", \" \"),\n",
    "    (r\"\\b(?:GENESIS[-\\s]*FS\\s+CARD\\s+PAYMENT)\\b\", \" \"),\n",
    "    (r\"\\bBILL\\s+PAYMENT\\b\", \" \"),\n",
    "    (r\"\\b(?:US|WA)\\s+CARD\\s+PURCHASE\\b\", \" \"),\n",
    "    (r\"-\\s*MEMO=\", \" \"),\n",
    "    (r\"(?:USA|US)$\", \" \"), # Remove USA or US at the end\n",
    "    (r\"\\s+FSP$\", \" \"),\n",
    "\n",
    "    # === 7) Phone numbers ===\n",
    "    (r\"\\b(?:\\d{3}-\\d{3}-\\d{4}|XXX-XXX-XXXX)\\b\", \" \"), # 800-555-1212\n",
    "    (r\"\\b\\d{3}-\\d{4}\\b\", \" \"), # 555-1212\n",
    "    (r\"\\b(?:\\d{3}\\s*){1,2}\\d{3}\\s*\\d{3,4}\\b\", \" \"), # 800 555 1212 or 1 800 555 1212\n",
    "    (r\"\\b#?\\s*\\d{3}-\\d{3}-\\d{1,4}\\s*(?:AM|PM)?\\b\", \" \"),\n",
    "\n",
    "    # === 8) URLs/domains ===\n",
    "    (r\"^\\.COM\\s+BILL\\b.*\", \" \"),\n",
    "\n",
    "    # === 9) State abbreviations ===\n",
    "    (STATE_REGEX, \" \"), # Remove standalone state codes\n",
    "\n",
    "    # === 10) Final Tidy (Punctuation) ===\n",
    "    (r\"[|%_=;\\\\/]+\", \" \"), # Remove misc separators\n",
    "    (r\"[-]{2,}\", \" \"), # Collapse multiple hyphens\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.2 s, sys: 186 ms, total: 22.4 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# First pass\n",
    "memos = df['memo'].astype(str).fillna('').str.upper()\n",
    "memos = memos.str.replace(r\"\\u00A0\", \" \", regex=True)\n",
    "memos = memos.str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "memos = memos.str.strip()\n",
    "\n",
    "for pattern, repl in REGEX_PRE:\n",
    "    memos = memos.str.replace(pattern, repl, regex=True)\n",
    "\n",
    "memos = memos.str.replace(NOISE_WORDS_REGEX, \" \", regex=True)\n",
    "memos = memos.str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "memos = memos.str.replace(r\"^[\\s-]+|[\\s-]+$\", \"\", regex=True)\n",
    "df['memo_pre'] = memos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REGEX_POST' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:8\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:3\u001b[0m, in \u001b[0;36mapply_regex\u001b[0;34m(memo)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REGEX_POST' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Second pass\n",
    "def apply_regex(memo):\n",
    "    for pattern in REGEX_POST:\n",
    "        match = pattern.match(memo)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return memo\n",
    "df['memo_post'] = df['memo_pre'].apply(apply_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('memos_P1.csv', index=False) # Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manually check\n",
    "# unique_df = df.drop_duplicates(subset='memo_post')\n",
    "# result = (\n",
    "#     unique_df[unique_df['memo_pre'].str.split().str.len() == 1]\n",
    "#     .sort_values(by='memo_pre')[100:200].to_string()\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'memo_post'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3431/3263582308.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'memo_post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7185\u001b[0m             )\n\u001b[1;32m   7186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7187\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7189\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7191\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7192\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'memo_post'"
     ]
    }
   ],
   "source": [
    "df.sample(10).sort_values(by='memo_post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['memo_pre'].str.contains('ELEVEN')].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants_clean = ['AMAZON', 'ALBERTSONS', 'ADOBE', 'SONIC', 'LIDL', 'AFTERPAY', 'ARBYS', 'ALDI', 'AUDIBLE', 'DOLLARTREE', 'LOWES', 'KROGER', 'DUNKIN', 'HP', 'PUBLIX', 'FRYS', 'SAFEWAY', 'DOORDASH', 'GOOGLE', 'WALMART', 'CHICK-FIL-A', 'SAMSCLUB', 'MICROSOFT',\n",
    "             'UBER', 'ULTA', 'H-E-B', 'VONS', 'CMSVEND', 'INSTACART', 'LYFT', 'TJMAXX', 'PETSMART', 'THORNTONS', 'PAYPAL', 'MAVERIK', 'WENDYS', 'MARSHALLS', 'ALLSUPS', 'SUNPASS', 'QVC', 'PRIZEPICKS', 'DDBR']\n",
    "\n",
    "merchants_punc = [\"DENNY'S\", 'WAL-MART', \"LOWE'S\", 'DOLLAR-GENERAL', '7-ELEVEN', \"WENDY'S\", \"ZAXBY'S\", 'FRYS-FOOD-DRG', \"BUC-EE'S\",\n",
    "                 \"BASHAS'\"]\n",
    "\n",
    "merchants_sites = ['AMAZON.COM', 'GODADDY.COM', 'CCBILL.COM']\n",
    "\n",
    "merchant_cats = ['', 'OVERDRAFT', 'WITHDRAWAL']\n",
    "\n",
    "multiples = [['AMAZON', 'AMAZON.COM', 'AMAZON PRIME'], ['LOWES', \"LOWE'S\"], ['BASKIN', 'DDBR']]\n",
    "\n",
    "merchants = merchants_clean + merchants_punc + merchants_sites + merchant_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for memo in df[df['memo_post'].str.split().str.len() == 1]['memo_post'].value_counts().sort_values(ascending=False).index:\n",
    "    if memo not in merchants:\n",
    "        print(memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['memo_post'] == '']#.iloc[0]['memo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['memo_post'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = r\"^[A-Z0-9\\.]+\\s*\\*\\s*[A-Z\\s0-9'.-]+\"\n",
    "\n",
    "prefix_star_merchants = df[df['memo_pre'].str.contains(regex_pattern, na=False)]\n",
    "prefix_star_merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(prefix_star_merchants['memo_pre'].str.split('*').sample(100).sort_values().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Extract & Analyze N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p2 = pd.read_csv(\"memos_P1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[df['memo_post'].str.len() < 4]['memo_post'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(corpus: pd.Series, n_gram_range: tuple, top_n: int = 200):\n",
    "    print(f\"Analyzing {n_gram_range} n-grams...\")\n",
    "    vec = CountVectorizer(\n",
    "        ngram_range=n_gram_range,\n",
    "        stop_words='english',\n",
    "        max_features=None  # We want to count all n-grams first\n",
    "    ).fit(corpus)\n",
    "    \n",
    "    # Get the counts\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    # Sum the counts for each n-gram\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    \n",
    "    # Map n-grams to their frequencies\n",
    "    words_freq = [\n",
    "        (word, sum_words[0, idx]) \n",
    "        for word, idx in vec.vocabulary_.items()\n",
    "    ]\n",
    "    \n",
    "    # Sort by frequency (descending)\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = df_p2['memo_post'].fillna('')\n",
    "print(f\"Analyzing {len(corpus)} cleaned memos...\")\n",
    "# Get the top 200 of each n-gram type\n",
    "top_1grams = top_ngrams(corpus, n_gram_range=(1, 1), top_n=200)\n",
    "top_2grams = top_ngrams(corpus, n_gram_range=(2, 2), top_n=200)\n",
    "top_3grams = top_ngrams(corpus, n_gram_range=(3, 3), top_n=200)\n",
    "print(f\"--- N-gram Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_1grams.sort(key=lambda x: x[0])\n",
    "ngrams_1 = []\n",
    "for ngram, value in top_1grams:\n",
    "    if ngram.upper() not in NOISE_WORDS:\n",
    "        ngrams_1 += [ngram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1 grams to find prefixes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
