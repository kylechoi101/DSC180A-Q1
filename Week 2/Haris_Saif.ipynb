{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Preprocessing with Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['git', 'pull'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run(['git', 'pull'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main a89c3f8] regex\n",
      " 1 file changed, 5050 insertions(+), 5584 deletions(-)\n",
      " rewrite Week 2/Haris_Saif.ipynb (61%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['git', 'commit', '-m', 'regex'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['git', 'add', 'Haris_Saif.ipynb'])\n",
    "subprocess.run(['git', 'commit', '-m', 'regex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in = pd.read_csv(\"memos.csv\")\n",
    "df = df_in.sample(100_000).copy()\n",
    "row_count = df.size\n",
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  BAY SUPERMARKET MIAMI BEACH FL 07/27\n",
       "1                                 Dd Doordash Adalberto\n",
       "2                                         ET'S XXXXXXXX\n",
       "3     OMAR'S 01-29 HARRISONBURG VA XXXX DEBIT CARD P...\n",
       "4                    POS EXA Wal-Mart Super OGALLALA NE\n",
       "5     POS Purchase Non-PIN Little Caesars XXXX-00 TN...\n",
       "6     POS Purchase With PIN SAMS CLUB #XXXX KNOXVILL...\n",
       "7     PURCHASE AUTHORIZED ON 03/07 GREAT WALL ALMA G...\n",
       "8     PURCHASE AUTHORIZED ON 05/25 TACO BELL XXXXXX ...\n",
       "9     PURCHASE AUTHORIZED ON 06/25 CASH APP*JEFFERY ...\n",
       "10    PURCHASE AUTHORIZED ON 10/24 CASH APP*SASHA TU...\n",
       "11    PURCHASE AUTHORIZED ON 11/09 SQ *HANDEL'S HOME...\n",
       "12    PURCHASE AUTHORIZED ON 11/13 FIVE LOAVES IRVIN...\n",
       "13    PURCHASE AUTHORIZED ON 11/27 VOWELLS MARKETPLA...\n",
       "14    Point of Sale Debit L340 DATE 03-23 Charrons D...\n",
       "15    RECURRING PAYMENT AUTHORIZED ON 06/28 OTT* FUL...\n",
       "16    RECURRING PAYMENT AUTHORIZED ON 11/10 COASTAL ...\n",
       "17    SLIM CHICKENS XXXX 04-20 MD XXXX DEBIT CARD PU...\n",
       "18    Sun Country Cleane 10-19 FL XXXX DEBIT CARD PU...\n",
       "19                    THE MEXICO CAFE TEMECULA CA 07/28\n",
       "20              TRUITT & WHITE LUMBER BERKELEY CA 07/11\n",
       "21    UBER   *EATS AMSTERDAM                       0...\n",
       "22    Visa Checking CASH APP*BRYAN NISS XXXXXXXXXX C...\n",
       "23    Withdrawal CONSUMER DEBIT / APPLE.COM/BILL XXX...\n",
       "24    XXXXXX CHK PURCH SIG RENT ONE ONLINE 314-XXXXX...\n",
       "Name: memo, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['memo'].sample(25).sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Regex Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_LIST = [\n",
    "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \n",
    "    \"(?<!\\.)CO(?!['`])\", # Negative lookbehind/ahead for CO (e.g., not .CO or COSTCO)\n",
    "    \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \"HI\", \"IA\", \n",
    "    \"ID\", \"IL\", \n",
    "    \"IN(?!\\\\s+N\\\\s+OUT\\\\s+BURGER)\", # Negative lookahead for IN (not IN N OUT BURGER)\n",
    "    \"KS\", \"KY\",\n",
    "    \"(?<!['`])LA(?!\\\\s+HACIENDA|\\\\s+FITNESS|\\\\s+LA'S|['`])\", # Negative lookaheads for LA\n",
    "    \"MA\", \"MD\", \n",
    "    \"ME(?!\\\\s+DIA)\", # Negative lookahead for ME (not ME DIA)\n",
    "    \"MI\", \"MN\", \"MO(?!['`])\", \n",
    "    \"MS\", \"MT\", \"NC\", \"ND\", \"NE\", \"NH\", \"NJ\", \"NM\", \"NV\", \"NY\", \n",
    "    \"OH\", \"OK\", \"OR\", \n",
    "    \"PA(?!['`])\", \n",
    "    \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VA\", \"VT\", \"WA\", \n",
    "    \"WI\", \"WV\", \"WY\"\n",
    "]\n",
    "STATE_REGEX = r\"\\b(\" + \"|\".join(STATE_LIST) + r\")\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From 1-gram\n",
    "NOISE_WORDS = [\n",
    "    \"DBT\", \"PURCH\", \"TRANSACTION\", \"PMT\", \"PMTS\", \"HTTPSWWW\", \"WWW\", \"CONSUMER\", \n",
    "    \"CKCD\", \"VENDING\", \"SUPERCENTER\", \"SUPERC\", \"STORE\", \"STORES\", \"RESTAURANT\", \n",
    "    \"PROTECTION\", \"PARKING\", \"GRILL\", \"MARKET\", \"LIQUOR\", \"LIQUORS\", \"GROCERY\", \n",
    "    \"FOOD\", \"FOODS\", \"DIGITAL\", \"DIGIT\", \"DELI\", \"COFFEE\", \"CITY\", \"CENTER\", \n",
    "    \"CAFE\", \"BUSINESS\", \"BEAUTY\", \"BAR\", \"STREET\"\n",
    "]\n",
    "NOISE_WORDS_REGEX = r\"\\b(\" + \"|\".join(NOISE_WORDS) + r\")\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_POST = [\n",
    "    # --- Specific/Tricky Rules ---\n",
    "    # This greedily finds the *last* instance of GODADDY.COM or GODADD\n",
    "    re.compile(r\".*(GODADDY\\.COM|GODADD)\\b.*\"),\n",
    "    \n",
    "    # Rules for 'PAYPAL [MERCHANT] INTERNET PAYMENT' format\n",
    "    re.compile(r\"^PAYPAL\\s+([A-Z\\s0-9'.*-]+?)\\s+(?:INTERNET\\s+PAYMENT|COID.*)?.*$\"),\n",
    "\n",
    "    # Rules for '[MERCHANT] ... PAYPAL' format\n",
    "    re.compile(r\"^([A-Z\\s0-9'.*-]+?)\\s+PAYPAL.*$\"),\n",
    "\n",
    "    # --- Standardized Prefix Rules ---\n",
    "    # (Capture group is placed *after* the prefix)\n",
    "    re.compile(r\"^TARGET\\b(.*)\"), # Special case for TARGET, might capture store #\n",
    "    re.compile(r\"^ACI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^KING\\s*#\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ACE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^7-ELEVEN\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^7\\s+11\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZG\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^YSI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^DD\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CCI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PY\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ANC\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^J2\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^OSP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PL\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^RTI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^FSP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    \n",
    "    # === DDBR RULE ===\n",
    "    # Captures DDBR as the merchant, ignoring location info after #\n",
    "    re.compile(r\"^(DDBR)\\s*\\*?#?.*\"), \n",
    "\n",
    "    re.compile(r\"^PT\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CHR\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^USA\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"), # Corrected\n",
    "    re.compile(r\"^SP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    \n",
    "    # === TST RULES ===\n",
    "    # Specific: (POS) TST* MERCHANT [TWO WORD CITY] ON ##...\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s0-9'.-]+?)\\s+[A-Z]{2,}\\s+[A-Z]{2,}\\s+ON\\s*##.*\"),\n",
    "    # Specific: (POS) TST* MERCHANT [ONE WORD CITY] ON ##...\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s0-9'.-]+?)\\s+[A-Z]{2,}\\s+ON\\s*##.*\"),\n",
    "    # Fallback: (POS) TST* MERCHANT...\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"), \n",
    "    \n",
    "    re.compile(r\"^CKE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^SQ\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^SP\\+AFF\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^IC\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"), # Corrected\n",
    "    re.compile(r\"^SIE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PAYPAL\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "\n",
    "    # --- Specific '*' prefix rules (already correct) ---\n",
    "    re.compile(r\"^AMS\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZSK\\s*\\*+ZSK\\s*\\*+([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZSK\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CCM\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZIP\\.CO\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^XSOLLA\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^NOR\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "\n",
    "    \n",
    "    # --- Generic Fallback Rules ---\n",
    "    # These rules try to find the merchant *before* a common delimiter.\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME * JUNK\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s*\\*.*\"), # Corrected 09 -> 0-9\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME # JUNK\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s*#.*\"), # Corrected 09 -> 0-9\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME - Exp\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s+-\\s+EXP.*\"), # Corrected 09 -> 0-9\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_POST = [\n",
    "    # --- Specific/Tricky Rules ---\n",
    "    # This greedily finds the *last* instance of GODADDY.COM or GODADD\n",
    "    re.compile(r\".*(GODADDY\\.COM|GODADD)\\b.*\"),\n",
    "    \n",
    "    # Rules for 'PAYPAL [MERCHANT] INTERNET PAYMENT' format\n",
    "    re.compile(r\"^PAYPAL\\s+([A-Z\\s0-9'.*-]+?)\\s+(?:INTERNET\\s+PAYMENT|COID.*)?.*$\"),\n",
    "\n",
    "    # Rules for '[MERCHANT] ... PAYPAL' format\n",
    "    re.compile(r\"^([A-Z\\s0-9'.*-]+?)\\s+PAYPAL.*$\"),\n",
    "\n",
    "    # --- Standardized Prefix Rules ---\n",
    "    # (Capture group is placed *after* the prefix)\n",
    "    re.compile(r\"^TARGET\\b(.*)\"), # Special case for TARGET, might capture store #\n",
    "    re.compile(r\"^ACI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^KING\\s*#\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ACE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^7-ELEVEN\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^7\\s+11\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZG\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^YSI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "\n",
    "    # === UPDATED RULE ORDER ===\n",
    "    # Specific DDBR rule must come *before* the general DD rule\n",
    "    re.compile(r\"^(DDBR)\\s*\\*?#?.*\"), \n",
    "    \n",
    "    re.compile(r\"^DD\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CCI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PY\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ANC\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^J2\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^OSP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PL\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^RTI\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^FSP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    # re.compile(r\"^DD\\s*BR\\s*\\*?#?\\s*([A-Z\\s0-9'.-]+).*\"), # Removed, redundant\n",
    "    re.compile(r\"^PT\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CHR\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^USA\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^SP\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    \n",
    "    # === TST RULES ===\n",
    "    # Specific: (POS) TST* MERCHANT [TWO WORD CITY] ON ##...\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s0-9'.-]+?)\\s+[A-Z]{2,}\\s+[A-Z]{2,}\\s+ON\\s*##.*\"),\n",
    "    # Specific: (POS) TST* MERCHANT [ONE WORD CITY] ON ##...\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s0-9'.-]+?)\\s+[A-Z]{2,}\\s+ON\\s*##.*\"),\n",
    "    # Fallback: (POS) TST* MERCHANT...\n",
    "    re.compile(r\"^(?:POS\\s+)?TST\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"), \n",
    "    \n",
    "    re.compile(r\"^CKE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^SQ\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^SP\\+AFF\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^IC\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"), \n",
    "    re.compile(r\"^SIE\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^PAYPAL\\s*\\*?\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "\n",
    "    # --- Specific '*' prefix rules (already correct) ---\n",
    "    re.compile(r\"^AMS\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZSK\\s*\\*+ZSK\\s*\\*+([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZSK\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^CCM\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^ZIP\\.CO\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^XSOLLA\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "    re.compile(r\"^NOR\\s*\\*+\\s*([A-Z\\s0-9'.-]+).*\"),\n",
    "\n",
    "    \n",
    "    # --- Generic Fallback Rules ---\n",
    "    # These rules try to find the merchant *before* a common delimiter.\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME * JUNK\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s*\\*.*\"),\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME # JUNK\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s*#.*\"),\n",
    "    \n",
    "    # Pattern for \"MERCHANT NAME - Exp\"\n",
    "    re.compile(r\"^([A-Z\\s'.-][A-Z\\s0-9'.-]*?)\\s+-\\s+EXP.*\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'REGEX_PRE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:7\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'REGEX_PRE' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# First pass\n",
    "memos = df['memo'].astype(str).fillna('').str.upper()\n",
    "memos = memos.str.replace(r\"\\u00A0\", \" \", regex=True)\n",
    "memos = memos.str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "memos = memos.str.strip()\n",
    "\n",
    "for pattern, repl in REGEX_PRE:\n",
    "    memos = memos.str.replace(pattern, repl, regex=True)\n",
    "        \n",
    "memos = memos.str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "memos = memos.str.replace(r\"^[\\s-]+|[\\s-]+$\", \"\", regex=True)\n",
    "df['memo_pre'] = memos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'memo_pre'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'memo_pre'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:8\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'memo_pre'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Second pass\n",
    "def apply_regex(memo):\n",
    "    for pattern in REGEX_POST:\n",
    "        match = pattern.match(memo)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    return memo\n",
    "df['memo_post'] = df['memo_pre'].apply(apply_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('memos_P1.csv', index=False) # Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manually check\n",
    "# unique_df = df.drop_duplicates(subset='memo_post')\n",
    "# result = (\n",
    "#     unique_df[unique_df['memo_pre'].str.split().str.len() == 1]\n",
    "#     .sort_values(by='memo_pre')[100:200].to_string()\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'memo_post'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2196/3263582308.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'memo_post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   7185\u001b[0m             )\n\u001b[1;32m   7186\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7187\u001b[0m             \u001b[0;31m# len(by) == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7189\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7191\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7192\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'memo_post'"
     ]
    }
   ],
   "source": [
    "df.sample(10).sort_values(by='memo_post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchants_clean = ['AMAZON', 'ALBERTSONS', 'ADOBE', 'SONIC', 'LIDL', 'AFTERPAY', 'ARBYS', 'ALDI', 'AUDIBLE', 'DOLLARTREE', 'LOWES', 'KROGER', 'DUNKIN', 'HP', 'PUBLIX', 'FRYS', 'SAFEWAY', 'DOORDASH', 'GOOGLE', 'WALMART', 'CHICK-FIL-A', 'SAMSCLUB', 'MICROSOFT',\n",
    "             'UBER', 'ULTA', 'H-E-B', 'VONS', 'CMSVEND', 'INSTACART', 'LYFT', 'TJMAXX', 'PETSMART', 'THORNTONS', 'PAYPAL', 'MAVERIK', 'WENDYS', 'MARSHALLS', 'ALLSUPS', 'SUNPASS', 'QVC', 'PRIZEPICKS', 'DDBR']\n",
    "\n",
    "merchants_punc = [\"DENNY'S\", 'WAL-MART', \"LOWE'S\", 'DOLLAR-GENERAL', '7-ELEVEN', \"WENDY'S\", \"ZAXBY'S\", 'FRYS-FOOD-DRG', \"BUC-EE'S\",\n",
    "                 \"BASHAS'\"]\n",
    "\n",
    "merchants_sites = ['AMAZON.COM', 'GODADDY.COM', 'CCBILL.COM']\n",
    "\n",
    "merchant_cats = ['', 'OVERDRAFT', 'WITHDRAWAL']\n",
    "\n",
    "multiples = [['AMAZON', 'AMAZON.COM', 'AMAZON PRIME'], ['LOWES', \"LOWE'S\"], ['BASKIN', 'DDBR']]\n",
    "\n",
    "merchants = merchants_clean + merchants_punc + merchants_sites + merchant_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for memo in df[df['memo_post'].str.split().str.len() == 1]['memo_post'].value_counts().sort_values(ascending=False).index:\n",
    "    if memo not in merchants:\n",
    "        print(memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['memo_post'] == 'TARGET']#.iloc[0]['memo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['memo_pre'].str.contains('TARGET')].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df['memo_post'].str.contains('COM')) | (df['memo_post'].str.contains('WWW')) | (df['memo_post'].str.contains('HTTP'))].sample(30)['memo_post']#.iloc[0]['memo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['memo_post'].str.contains('APPLE')) & (df['memo_post'].str.contains('COM'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_pattern = r\"^[A-Z0-9\\.]+\\s*\\*\\s*[A-Z\\s0-9'.-]+\"\n",
    "\n",
    "prefix_star_merchants = df[df['memo_pre'].str.contains(regex_pattern, na=False)]\n",
    "prefix_star_merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(prefix_star_merchants['memo_pre'].str.split('*').sample(100).sort_values().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Extract & Analyze N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p2 = pd.read_csv(\"memos_P1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['memo_post'].str.len() < 4]['memo_post'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ngrams(corpus: pd.Series, n_gram_range: tuple, top_n: int = 200):\n",
    "    print(f\"Analyzing {n_gram_range} n-grams...\")\n",
    "    vec = CountVectorizer(\n",
    "        ngram_range=n_gram_range,\n",
    "        stop_words='english',\n",
    "        max_features=None  # We want to count all n-grams first\n",
    "    ).fit(corpus)\n",
    "    \n",
    "    # Get the counts\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    # Sum the counts for each n-gram\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    \n",
    "    # Map n-grams to their frequencies\n",
    "    words_freq = [\n",
    "        (word, sum_words[0, idx]) \n",
    "        for word, idx in vec.vocabulary_.items()\n",
    "    ]\n",
    "    \n",
    "    # Sort by frequency (descending)\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# corpus = df_p2['memo_post'].fillna('')\n",
    "# print(f\"Analyzing {len(corpus)} cleaned memos...\")\n",
    "# # Get the top 200 of each n-gram type\n",
    "# top_1grams = top_ngrams(corpus, n_gram_range=(1, 1), top_n=200)\n",
    "# top_2grams = top_ngrams(corpus, n_gram_range=(2, 2), top_n=200)\n",
    "# top_3grams = top_ngrams(corpus, n_gram_range=(3, 3), top_n=200)\n",
    "# print(f\"--- N-gram Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_1grams.sort(reverse=True\n",
    "    \n",
    ")\n",
    "top_1grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_2grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_3grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 1 grams to find prefixes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
