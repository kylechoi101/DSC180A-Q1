{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Outflows = pd.read_parquet(\"q1-ucsd-outflows.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OK: shapes & autograd paths are good.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1177.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from datarater import InnerClassifier, DataRater\n",
    "\n",
    "# fake dataset sizes\n",
    "vocab_size = 500\n",
    "num_classes = 5\n",
    "pad_id = 0\n",
    "\n",
    "# instantiate models\n",
    "inner = InnerClassifier(vocab_size=vocab_size, num_classes=num_classes, pad_id=pad_id)\n",
    "dr = DataRater(vocab_size=vocab_size, pad_id=pad_id)\n",
    "\n",
    "# create random inputs\n",
    "X = torch.randint(0, vocab_size, (8, 20))\n",
    "Y = torch.randint(0, num_classes, (8,))\n",
    "\n",
    "# forward passes\n",
    "scores = dr(X)                 # (8,)\n",
    "assert scores.shape == (8,)\n",
    "per_ex = inner(X, Y)           # (8,)\n",
    "\n",
    "# compute weighted loss\n",
    "loss = (torch.softmax(scores, dim=0) * per_ex).sum()\n",
    "loss.backward(create_graph=True)\n",
    "\n",
    "print(\"✅ OK: shapes & autograd paths are good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ttest_ind\n",
    "inflow = pd.read_parquet('q1-ucsd-inflows.pqt')\n",
    "outflow = pd.read_parquet('q1-ucsd-outflows.pqt')\n",
    "consumers_both = sorted(set(inflow[\"prism_consumer_id\"]).intersection(outflow[\"prism_consumer_id\"]))\n",
    "\n",
    "#80-20 train test split\n",
    "train_ids, test_ids = train_test_split(consumers_both, test_size=0.2, random_state=42)\n",
    "\n",
    "inflow_train = inflow[inflow[\"prism_consumer_id\"].isin(train_ids)]\n",
    "inflow_test  = inflow[inflow[\"prism_consumer_id\"].isin(test_ids)]\n",
    "\n",
    "outflow_train = outflow[outflow[\"prism_consumer_id\"].isin(train_ids)]\n",
    "outflow_test  = outflow[outflow[\"prism_consumer_id\"].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 0) Inputs: a pandas DataFrame with columns: [\"memo\", \"category\", \"amount\", \"posted_date\"] ----\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# base dataframe\n",
    "df = outflow_train[outflow_train['memo'] != outflow_train['category']].copy()     # (same as before)\n",
    "df['posted_date'] = pd.to_datetime(df['posted_date'], errors='coerce')            # <<< MODIFIED\n",
    "df = df.dropna(subset=['memo', 'category', 'amount', 'posted_date'])              # <<< MODIFIED\n",
    "\n",
    "# ---- helpers for aux features (amount + posted_date) ----\n",
    "def cyclical_encode(val, period):\n",
    "    angle = 2 * np.pi * (val / period)\n",
    "    return np.sin(angle), np.cos(angle)\n",
    "\n",
    "# ---- 1) Tokenization & vocab (unchanged) ----\n",
    "def basic_tokenize(text: str) -> List[str]:\n",
    "    return text.strip().lower().split()\n",
    "\n",
    "def build_vocab(texts: List[str], min_freq: int = 1) -> Dict[str, int]:\n",
    "    counter = Counter()\n",
    "    for t in texts:\n",
    "        counter.update(basic_tokenize(t))\n",
    "    stoi = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "    for tok, freq in counter.items():\n",
    "        if freq >= min_freq and tok not in stoi:\n",
    "            stoi[tok] = len(stoi)\n",
    "    return stoi\n",
    "\n",
    "def encode_text(text: str, stoi: Dict[str, int]) -> List[int]:\n",
    "    return [stoi.get(tok, stoi[\"<unk>\"]) for tok in basic_tokenize(text)]\n",
    "\n",
    "\n",
    "# ---- 2) Labels (unchanged) ----\n",
    "def build_label_map(labels: List[str]) -> Dict[str, int]:\n",
    "    classes = sorted(set(labels))\n",
    "    return {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "def build_inverse_label_map(ltoi: Dict[str, int]) -> Dict[int, str]:\n",
    "    return {i: c for c, i in ltoi.items()}\n",
    "\n",
    "\n",
    "# ---- 3) Split FIRST, then fit aux stats on train ONLY ----  # <<< MODIFIED\n",
    "perm = torch.randperm(len(df))\n",
    "cut  = int(0.8 * len(df))\n",
    "train_df = df.iloc[perm[:cut]].reset_index(drop=True)\n",
    "val_df   = df.iloc[perm[cut:]].reset_index(drop=True)\n",
    "\n",
    "# fit amount stats on train only (avoid leakage)\n",
    "_amt_mean = train_df['amount'].mean()\n",
    "_amt_std  = train_df['amount'].std(ddof=0) + 1e-8\n",
    "\n",
    "def add_aux_columns(_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    _df = _df.copy()\n",
    "    # amount features (use train stats)\n",
    "    _df['amount_z'] = (_df['amount'] - _amt_mean) / _amt_std\n",
    "    _df['amount_log_signed'] = np.sign(_df['amount']) * np.log1p(np.abs(_df['amount']))\n",
    "\n",
    "    # day-of-week and month cyclic encodings\n",
    "    dow = _df['posted_date'].dt.dayofweek.astype(int)\n",
    "    mon = (_df['posted_date'].dt.month.astype(int) - 1).clip(0, 11)\n",
    "    dow_sin, dow_cos = zip(*[cyclical_encode(v, 7)  for v in dow])\n",
    "    mon_sin, mon_cos = zip(*[cyclical_encode(v, 12) for v in mon])\n",
    "    _df['dow_sin'] = dow_sin; _df['dow_cos'] = dow_cos\n",
    "    _df['mon_sin'] = mon_sin; _df['mon_cos'] = mon_cos\n",
    "    return _df\n",
    "\n",
    "train_df = add_aux_columns(train_df)   # <<< MODIFIED\n",
    "val_df   = add_aux_columns(val_df)     # <<< MODIFIED\n",
    "\n",
    "AUX_COLS = ['amount_z', 'amount_log_signed', 'dow_sin', 'dow_cos', 'mon_sin', 'mon_cos']  # <<< MODIFIED\n",
    "AUX_DIM  = len(AUX_COLS)                                                                        # <<< MODIFIED\n",
    "\n",
    "\n",
    "# ---- 4) Build vocab from TRAIN ONLY, then labels from all ----  # <<< MODIFIED\n",
    "stoi = build_vocab(train_df[\"memo\"].astype(str).tolist(), min_freq=1)\n",
    "ltoi = build_label_map(pd.concat([train_df[\"category\"], val_df[\"category\"]], axis=0).astype(str).tolist())\n",
    "itol = build_inverse_label_map(ltoi)\n",
    "\n",
    "vocab_size = len(stoi)\n",
    "num_classes = len(ltoi)\n",
    "pad_id = stoi[\"<pad>\"]\n",
    "\n",
    "\n",
    "# ---- 5) Dataset + collate now return (tokens, AUX, label) ----  # <<< MODIFIED\n",
    "class MemoDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, stoi: Dict[str, int], ltoi: Dict[str, int], max_len: int = 64):\n",
    "        self.pad_id = stoi[\"<pad>\"]\n",
    "        self.samples: List[Tuple[List[int], torch.Tensor, int]] = []\n",
    "        for _, row in df.iterrows():\n",
    "            ids = encode_text(str(row[\"memo\"]), stoi)[:max_len]\n",
    "            aux = torch.tensor(row[AUX_COLS].values.astype(np.float32))  # (F,)\n",
    "            y   = ltoi[str(row[\"category\"])]\n",
    "            self.samples.append((ids, aux, y))\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_ids, aux, y = self.samples[idx]\n",
    "        return torch.tensor(x_ids, dtype=torch.long), aux, torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "def collate_batch(batch, pad_id=0):\n",
    "    xs, auxs, ys = zip(*batch)                                            # <<< MODIFIED\n",
    "    L = max(len(x) for x in xs) if xs else 1\n",
    "    X = torch.full((len(xs), L), pad_id, dtype=torch.long)\n",
    "    for i, x in enumerate(xs):\n",
    "        X[i, :len(x)] = x\n",
    "    AUX = torch.stack(auxs)                                              # <<< MODIFIED  (B, F)\n",
    "    return X, AUX, torch.stack(ys)                                       # <<< MODIFIED\n",
    "\n",
    "\n",
    "# ---- 6) Datasets / Loaders ----\n",
    "train_ds = MemoDataset(train_df, stoi, ltoi, max_len=64)\n",
    "val_ds   = MemoDataset(val_df,   stoi, ltoi, max_len=64)\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True,  collate_fn=lambda b: collate_batch(b, pad_id))  # <<< MODIFIED (batch size)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=256, shuffle=False, collate_fn=lambda b: collate_batch(b, pad_id))  # <<< MODIFIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n",
      "epoch 0  val_acc=0.980\n",
      "epoch 1  val_acc=0.982\n",
      "epoch 2  val_acc=0.982\n",
      "epoch 3  val_acc=0.983\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(total, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     acc \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m inner\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, Y \u001b[38;5;129;01min\u001b[39;00m train_dl:\n\u001b[0;32m---> 22\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, Y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m     opt_inner\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# DataRater weights (batch-softmax)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # MPS (Apple Silicon)\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = pick_device()\n",
    "print(\"Using:\", device)\n",
    "inner = InnerClassifier(vocab_size=vocab_size, num_classes=num_classes, pad_id=pad_id,\n",
    "                        d_model=128, n_layers=2, n_heads=4, max_seq_len=128).to(device)\n",
    "dr    = DataRater(vocab_size=vocab_size, pad_id=pad_id, d_model=128, max_seq_len=128).to(device)\n",
    "\n",
    "opt_inner = Adam(inner.parameters(), lr=2e-3)\n",
    "\n",
    "def train_one_epoch():\n",
    "    inner.train()\n",
    "    for X, Y in train_dl:\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        opt_inner.zero_grad(set_to_none=True)\n",
    "        # DataRater weights (batch-softmax)\n",
    "        scores  = dr(X)                               # (B,)\n",
    "        weights = torch.softmax(scores, dim=0)        # sum=1\n",
    "        per_ex  = inner(X, Y)                         # (B,)\n",
    "        loss    = (weights * per_ex).sum()\n",
    "        loss.backward()                               # no meta yet → no create_graph\n",
    "        opt_inner.step()\n",
    "\n",
    "def evaluate():\n",
    "    inner.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, Y in val_dl:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            probs = inner.predict_proba(X)\n",
    "            pred  = probs.argmax(dim=-1)\n",
    "            correct += (pred == Y).sum().item()\n",
    "            total   += Y.numel()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    train_one_epoch()\n",
    "    acc = evaluate()\n",
    "    print(f\"epoch {epoch}  val_acc={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prism_consumer_id</th>\n",
       "      <th>prism_account_id</th>\n",
       "      <th>memo</th>\n",
       "      <th>amount</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>2</td>\n",
       "      <td>acc_3</td>\n",
       "      <td>PURCHASE AUTHORIZED ON 03/11 POKE POKU HENDERS...</td>\n",
       "      <td>35.08</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>FOOD_AND_BEVERAGES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>2</td>\n",
       "      <td>acc_3</td>\n",
       "      <td>PURCHASE AUTHORIZED ON 10/01 LIQUOR CITY HENDE...</td>\n",
       "      <td>43.83</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>FOOD_AND_BEVERAGES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>2</td>\n",
       "      <td>acc_3</td>\n",
       "      <td>PURCHASE INTL AUTHORIZED ON 10/20 Rituals Cosm...</td>\n",
       "      <td>98.27</td>\n",
       "      <td>2021-10-21</td>\n",
       "      <td>GENERAL_MERCHANDISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>2</td>\n",
       "      <td>acc_3</td>\n",
       "      <td>Trader Joe''s</td>\n",
       "      <td>152.61</td>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>GROCERIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>2</td>\n",
       "      <td>acc_3</td>\n",
       "      <td>PURCHASE AUTHORIZED ON 05/28 VANS #174 LAS VEG...</td>\n",
       "      <td>81.17</td>\n",
       "      <td>2021-05-28</td>\n",
       "      <td>GENERAL_MERCHANDISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597457</th>\n",
       "      <td>5941</td>\n",
       "      <td>acc_9524</td>\n",
       "      <td>DEBIT CARD WITHDRAWAL PURCHASEAmazon Prime*TI4...</td>\n",
       "      <td>15.93</td>\n",
       "      <td>2023-01-16</td>\n",
       "      <td>GENERAL_MERCHANDISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597462</th>\n",
       "      <td>5941</td>\n",
       "      <td>acc_9524</td>\n",
       "      <td>POS WITHDRAWALAZ LOT QUIKTRIP XXXX XXXX E INDI...</td>\n",
       "      <td>25.00</td>\n",
       "      <td>2023-01-18</td>\n",
       "      <td>EDUCATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597465</th>\n",
       "      <td>5941</td>\n",
       "      <td>acc_9524</td>\n",
       "      <td>POS WITHDRAWALWAL-MART #XXXX XXXX E MCKELLIPS ...</td>\n",
       "      <td>3.68</td>\n",
       "      <td>2023-01-18</td>\n",
       "      <td>FOOD_AND_BEVERAGES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597468</th>\n",
       "      <td>5941</td>\n",
       "      <td>acc_9524</td>\n",
       "      <td>WITHDRAWAL Salt River ProjeTYPE: ONLINE PMT CO...</td>\n",
       "      <td>90.00</td>\n",
       "      <td>2023-01-20</td>\n",
       "      <td>FOOD_AND_BEVERAGES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597476</th>\n",
       "      <td>5941</td>\n",
       "      <td>acc_9524</td>\n",
       "      <td>POS WITHDRAWALFRYS-FOOD-DRG #1 435 S. E MESA A...</td>\n",
       "      <td>7.74</td>\n",
       "      <td>2023-01-21</td>\n",
       "      <td>FOOD_AND_BEVERAGES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1044281 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         prism_consumer_id prism_account_id  \\\n",
       "646                      2            acc_3   \n",
       "651                      2            acc_3   \n",
       "657                      2            acc_3   \n",
       "658                      2            acc_3   \n",
       "660                      2            acc_3   \n",
       "...                    ...              ...   \n",
       "2597457               5941         acc_9524   \n",
       "2597462               5941         acc_9524   \n",
       "2597465               5941         acc_9524   \n",
       "2597468               5941         acc_9524   \n",
       "2597476               5941         acc_9524   \n",
       "\n",
       "                                                      memo  amount  \\\n",
       "646      PURCHASE AUTHORIZED ON 03/11 POKE POKU HENDERS...   35.08   \n",
       "651      PURCHASE AUTHORIZED ON 10/01 LIQUOR CITY HENDE...   43.83   \n",
       "657      PURCHASE INTL AUTHORIZED ON 10/20 Rituals Cosm...   98.27   \n",
       "658                                          Trader Joe''s  152.61   \n",
       "660      PURCHASE AUTHORIZED ON 05/28 VANS #174 LAS VEG...   81.17   \n",
       "...                                                    ...     ...   \n",
       "2597457  DEBIT CARD WITHDRAWAL PURCHASEAmazon Prime*TI4...   15.93   \n",
       "2597462  POS WITHDRAWALAZ LOT QUIKTRIP XXXX XXXX E INDI...   25.00   \n",
       "2597465  POS WITHDRAWALWAL-MART #XXXX XXXX E MCKELLIPS ...    3.68   \n",
       "2597468  WITHDRAWAL Salt River ProjeTYPE: ONLINE PMT CO...   90.00   \n",
       "2597476  POS WITHDRAWALFRYS-FOOD-DRG #1 435 S. E MESA A...    7.74   \n",
       "\n",
       "        posted_date             category  \n",
       "646      2021-03-15   FOOD_AND_BEVERAGES  \n",
       "651      2021-10-04   FOOD_AND_BEVERAGES  \n",
       "657      2021-10-21  GENERAL_MERCHANDISE  \n",
       "658      2021-04-14            GROCERIES  \n",
       "660      2021-05-28  GENERAL_MERCHANDISE  \n",
       "...             ...                  ...  \n",
       "2597457  2023-01-16  GENERAL_MERCHANDISE  \n",
       "2597462  2023-01-18            EDUCATION  \n",
       "2597465  2023-01-18   FOOD_AND_BEVERAGES  \n",
       "2597468  2023-01-20   FOOD_AND_BEVERAGES  \n",
       "2597476  2023-01-21   FOOD_AND_BEVERAGES  \n",
       "\n",
       "[1044281 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
